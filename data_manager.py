import pandas as pd
import numpy as np
from datetime import datetime
import json


def load_environmental_data(file_path, required_columns=None):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    –° –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û–ô –ö–û–î–ò–†–û–í–ö–û–ô
    """
    if required_columns is None:
        required_columns = ['so2', 'no2', 'date']

    validation_report = {
        'status': 'success',
        'errors': [],
        'warnings': [],
        'records_loaded': 0,
        'data_period': None
    }

    try:
        print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {file_path}")

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û
        encodings = ['latin-1', 'cp1252', 'iso-8859-1', 'utf-8', 'windows-1252']

        data = None
        used_encoding = None

        for encoding in encodings:
            try:
                print(f"   –ü—Ä–æ–±—É–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É: {encoding}")
                data = pd.read_csv(file_path, encoding=encoding, low_memory=False)
                used_encoding = encoding
                print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π: {encoding}")
                break
            except UnicodeDecodeError as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ Unicode —Å {encoding}: {e}")
                continue
            except Exception as e:
                print(f"   ‚ùå –î—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞ —Å {encoding}: {e}")
                continue

        if data is None:
            validation_report['status'] = 'error'
            validation_report['errors'].append("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –Ω–∏ —Å –æ–¥–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π")
            return pd.DataFrame(), validation_report

        validation_report['records_loaded'] = len(data)
        validation_report['encoding_used'] = used_encoding

        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {used_encoding}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            validation_report['warnings'].append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
            print(f"‚ö† –í–Ω–∏–º–∞–Ω–∏–µ: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {data.columns.tolist()}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã
        if 'date' in data.columns:
            data['date'] = pd.to_datetime(data['date'], errors='coerce')
            invalid_dates = data['date'].isna().sum()
            if invalid_dates > 0:
                validation_report['warnings'].append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {invalid_dates} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞—Ç")
                data = data.dropna(subset=['date'])
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ date, –∏—â–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
            date_alternatives = ['sampling_date', 'timestamp', 'time']
            for alt in date_alternatives:
                if alt in data.columns:
                    print(f"üïê –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –∫–æ–ª–æ–Ω–∫—É –¥–∞—Ç—ã: {alt}")
                    data['date'] = pd.to_datetime(data[alt], errors='coerce')
                    invalid_dates = data['date'].isna().sum()
                    if invalid_dates > 0:
                        validation_report['warnings'].append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {invalid_dates} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞—Ç –≤ {alt}")
                        data = data.dropna(subset=['date'])
                    break

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        numeric_columns = ['so2', 'no2', 'rspm', 'spm', 'pm2_5']
        for col in numeric_columns:
            if col in data.columns:
                data[col] = pd.to_numeric(data[col], errors='coerce')
                invalid_values = data[col].isna().sum()
                if invalid_values > 0:
                    validation_report['warnings'].append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {invalid_values} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ {col}")

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö
        if len(data) > 0 and 'date' in data.columns:
            validation_report['data_period'] = {
                'start': data['date'].min().strftime('%Y-%m-%d'),
                'end': data['date'].max().strftime('%Y-%m-%d')
            }
            print(f"üìÖ –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {validation_report['data_period']}")

        return data, validation_report

    except Exception as e:
        validation_report['status'] = 'error'
        validation_report['errors'].append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return pd.DataFrame(), validation_report


def detect_anomalies_mad(data, pollutant_column, sensitivity='auto'):
    """
    –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –º–µ—Ç–æ–¥–æ–º MAD –¥–ª—è –≤–∞—à–µ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
    """

    print(f"üîç –ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ: {pollutant_column}")

    if pollutant_column not in data.columns:
        print(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ {pollutant_column} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")
        print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {data.columns.tolist()}")
        return data, {'anomalies_detected': 0, 'error': f'–ö–æ–ª–æ–Ω–∫–∞ {pollutant_column} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}

    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–±–æ—Ç—ã
    working_data = data.copy()

    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Ü–µ–ª–µ–≤–æ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ
    initial_count = len(working_data)
    working_data = working_data.dropna(subset=[pollutant_column])
    removed_nulls = initial_count - len(working_data)

    if removed_nulls > 0:
        print(f"‚ö† –£–¥–∞–ª–µ–Ω–æ {removed_nulls} –∑–∞–ø–∏—Å–µ–π —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ –≤ {pollutant_column}")

    if len(working_data) == 0:
        print(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤ –∫–æ–ª–æ–Ω–∫–µ {pollutant_column}")
        return data, {'anomalies_detected': 0, 'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'}

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ—Ä–æ–≥–∞
    sensitivity_map = {'low': 3.5, 'medium': 3.0, 'high': 2.5}
    threshold = sensitivity_map.get(sensitivity, 3.0)

    if sensitivity == 'auto':
        cv = working_data[pollutant_column].std() / working_data[pollutant_column].mean()
        threshold = 3.5 if cv > 1.0 else 2.8 if cv > 0.5 else 2.5

    # –†–∞—Å—á–µ—Ç MAD
    median = working_data[pollutant_column].median()
    mad = (working_data[pollutant_column] - median).abs().median()

    if mad == 0:
        mad = working_data[pollutant_column].std() / 1.4826

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü
    lower_bound = median - threshold * mad
    upper_bound = median + threshold * mad

    # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
    anomalies_mask = (working_data[pollutant_column] < lower_bound) | (working_data[pollutant_column] > upper_bound)
    clean_data = working_data[~anomalies_mask]
    anomalies_data = working_data[anomalies_mask]

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        'anomalies_detected': len(anomalies_data),
        'anomaly_percentage': (len(anomalies_data) / len(working_data)) * 100,
        'threshold_used': threshold,
        'median': median,
        'bounds': {'lower': lower_bound, 'upper': upper_bound}
    }

    print(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {stats['anomalies_detected']} –∞–Ω–æ–º–∞–ª–∏–π ({stats['anomaly_percentage']:.1f}%)")

    return clean_data, stats


def normalize_measurements(data):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–º–µ—Ä–µ–Ω–∏–π
    """
    print("üìä –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")

    normalized_data = data.copy()

    numeric_columns = ['so2', 'no2', 'rspm', 'spm', 'pm2_5']

    for col in numeric_columns:
        if col in normalized_data.columns:
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π
            if normalized_data[col].isna().sum() > 0:
                median_val = normalized_data[col].median()
                normalized_data[col] = normalized_data[col].fillna(median_val)

            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            min_val = normalized_data[col].min()
            max_val = normalized_data[col].max()
            if max_val > min_val:
                normalized_data[f'{col}_normalized'] = (normalized_data[col] - min_val) / (max_val - min_val)

    print("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return normalized_data


def prepare_analysis_dataset(data):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    """
    print("üõ† –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")

    analysis_data = data.copy()

    # –ë–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    base_columns = ['date', 'so2', 'no2', 'rspm', 'spm']
    available_columns = [col for col in base_columns if col in analysis_data.columns]

    analysis_data = analysis_data[available_columns]

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –µ—Å–ª–∏ –µ—Å—Ç—å
    if 'date' in analysis_data.columns:
        analysis_data = analysis_data.sort_values('date')

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    for col in available_columns:
        if col != 'date' and col in analysis_data.columns:
            if analysis_data[col].isna().sum() > 0:
                analysis_data[col] = analysis_data[col].fillna(analysis_data[col].median())

    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {len(analysis_data)} –∑–∞–ø–∏—Å–µ–π, {len(available_columns)} –∫–æ–ª–æ–Ω–æ–∫")
    return analysis_data

def detect_anomalies_iqr(data, pollutant_column):
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –º–µ—Ç–æ–¥–æ–º IQR"""
    if pollutant_column not in data.columns:
        return data, {'error': f'–ö–æ–ª–æ–Ω–∫–∞ {pollutant_column} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}

    working_data = data.copy()
    working_data = working_data.dropna(subset=[pollutant_column])

    if len(working_data) == 0:
        return data, {'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'}

    Q1 = working_data[pollutant_column].quantile(0.25)
    Q3 = working_data[pollutant_column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    anomalies_mask = (working_data[pollutant_column] < lower_bound) | (working_data[pollutant_column] > upper_bound)
    clean_data = working_data[~anomalies_mask]
    anomalies_data = working_data[anomalies_mask]

    stats = {
        'anomalies_detected': len(anomalies_data),
        'anomaly_percentage': (len(anomalies_data) / len(working_data)) * 100,
        'bounds': {'lower': lower_bound, 'upper': upper_bound}
    }

    return clean_data, stats

def detect_anomalies_zscore(data, pollutant_column, threshold=3):
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –º–µ—Ç–æ–¥–æ–º Z-score"""
    if pollutant_column not in data.columns:
        return data, {'error': f'–ö–æ–ª–æ–Ω–∫–∞ {pollutant_column} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}

    working_data = data.copy()
    working_data = working_data.dropna(subset=[pollutant_column])

    if len(working_data) == 0:
        return data, {'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'}

    mean = working_data[pollutant_column].mean()
    std = working_data[pollutant_column].std()

    z_scores = (working_data[pollutant_column] - mean) / std
    anomalies_mask = abs(z_scores) > threshold

    clean_data = working_data[~anomalies_mask]
    anomalies_data = working_data[anomalies_mask]

    stats = {
        'anomalies_detected': len(anomalies_data),
        'anomaly_percentage': (len(anomalies_data) / len(working_data)) * 100,
        'threshold_used': threshold
    }

    return clean_data, stats