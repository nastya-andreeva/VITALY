import data_manager as dm
import analysis_core as ac
import pandas as pd
import json
import os
import numpy as np


class NumpyEncoder(json.JSONEncoder):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è numpy —Ç–∏–ø–æ–≤"""

    def default(self, obj):
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, pd.Timestamp):
            return obj.isoformat()
        elif pd.isna(obj):
            return None
        return super().default(obj)


def run_complete_analysis(file_path):
    """
    –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ GUI

    Parameters:
    file_path (str): –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞–Ω–Ω—ã—Ö

    Returns:
    dict: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """

    print(f"üîÑ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Ñ–∞–π–ª–∞: {file_path}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    raw_data, validation_report = dm.load_environmental_data(file_path)

    if raw_data.empty:
        return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ'}

    results = {
        'validation_report': validation_report,
        'data_info': {
            'records_loaded': len(raw_data),
            'columns': raw_data.columns.tolist(),
            'period': validation_report.get('data_period')
        }
    }

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è
    numeric_columns = ['so2', 'no2', 'rspm', 'spm', 'pm2_5']
    target_pollutant = None

    for col in numeric_columns:
        if col in raw_data.columns and raw_data[col].notna().sum() > 1000:
            target_pollutant = col
            break

    if not target_pollutant:
        return {'error': '–ù–µ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö'}

    results['target_pollutant'] = target_pollutant

    # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    cleaned_data, anomalies_stats = dm.detect_anomalies_mad(
        raw_data, target_pollutant, sensitivity='auto'
    )

    results['anomalies_stats'] = anomalies_stats

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    analysis_data = dm.prepare_analysis_dataset(cleaned_data)

    # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
    if 'date' in analysis_data.columns:
        analysis_data['year'] = analysis_data['date'].dt.year
        yearly_avg = analysis_data.groupby('year')[target_pollutant].mean().reset_index()

        if len(yearly_avg) > 1:
            first_val = float(yearly_avg.iloc[0][target_pollutant])
            last_val = float(yearly_avg.iloc[-1][target_pollutant])
            change_percent = float(((last_val - first_val) / first_val) * 100)

            results['trend_analysis'] = {
                'overall_direction': '—Ä–æ—Å—Ç' if change_percent > 0 else '—Å–Ω–∏–∂–µ–Ω–∏–µ',
                'change_percentage': abs(change_percent),
                'first_year_avg': first_val,
                'last_year_avg': last_val,
                'years_analyzed': len(yearly_avg),
                'period': f"{yearly_avg['year'].min()}-{yearly_avg['year'].max()}"
            }

    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if target_pollutant in analysis_data.columns:
        stats = analysis_data[target_pollutant].describe()
        results['basic_statistics'] = {
            'mean': float(stats['mean']),
            'median': float(stats['50%']),
            'max': float(stats['max']),
            'min': float(stats['min']),
            'std': float(stats['std']),
            'count': int(stats['count'])
        }

    print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    return results


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
    results = run_complete_analysis('data/air_quality_data.csv')
    print(json.dumps(results, indent=2, ensure_ascii=False, cls=NumpyEncoder))